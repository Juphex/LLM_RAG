{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SslnlBaKPeWP"
   },
   "source": [
    "# Semantic Search with Embeddings on the Formula Student Rules\n",
    "\n",
    "This notebook demonstrates how to use Cohere's semantic search API to search through the Formula Student Rules using word embeddings.\n",
    "<br>\n",
    "The implementation is [1] based on this notebook. This could be seen as a standalone version but also as the first step of the preprocessing for a RAG pipeline.\n",
    "<br>\n",
    "<br>\n",
    "[1] https://github.com/cohere-ai/notebooks/blob/main/notebooks/Wikipedia_Semantic_Search_With_Cohere_Embeddings_Archives.ipynb?ref=txt.cohere.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUnwp2cYNnP0",
    "outputId": "6b4b299e-b08e-4893-a66f-da7933ee5a6f"
   },
   "outputs": [],
   "source": [
    "# Let's install cohere and HF datasets\n",
    "!pip install cohere datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8Pogz7gPQwg",
    "outputId": "06fc688b-3419-402b-a905-cc8a3b263060"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cohere\n",
    "\n",
    "# Add your cohere API key from https://dashboard.cohere.com/api-keys\n",
    "# Test key is enough for this example\n",
    "co = cohere.Client(\"\")\n",
    "# basic embedding model\n",
    "MODEL = \"embed-english-v2.0\"\n",
    "#Load at max 2000 documents + embeddings\n",
    "max_docs = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data, i.e. creating Question-Answer pairs or chunks. Whatever you want to name it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# find all titles and texts in this format:\n",
    "# \"\"\"\n",
    "#    AIP\n",
    "#\n",
    "# Anti Intrusion Plate\n",
    "# \"\"\"\n",
    "#\n",
    "# or in this format:\n",
    "# \"\"\"\n",
    "# A 1.2.6\n",
    "# \n",
    "# Vehicles of both classes can take part in an additional Driverless Cup (DC).\n",
    "# \"\"\"\n",
    "\n",
    "# read data/FS-Rules_2024_v1.1.0.txt \n",
    "with open('data/Rules_2024_v1.1.txt', 'r') as f:\n",
    "    rules = f.read()\n",
    "\n",
    "# Define a regex pattern to match titles and texts\n",
    "pattern = re.compile(r'([A-Z0-9\\s\\.-]+)\\n\\n((?:.*?\\n)+?)(?=\\n[A-Z0-9\\s\\.-]+|$)', re.DOTALL)\n",
    "\n",
    "# Find all matches in the text\n",
    "matches = pattern.findall(rules)\n",
    "\n",
    "# Extract titles and texts and filter based on length\n",
    "filtered_matches = [(title.strip(), text.strip()) for title, text in matches if len(title.strip()) > 3 or len(text.strip()) > 3]\n",
    "\n",
    "titles = [title for title, text in filtered_matches]\n",
    "texts = [text for title, text in filtered_matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to generate new Embedings - normally not necessary since they can be read from pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5546875, 0.9472656, 0.8847656, -0.52978516, 1.1357422]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Embed titles and texts with the given embedding model\n",
    "texts_embedded = co.embed(texts=texts, model=MODEL)\n",
    "titles_embedded = co.embed(texts=titles, model=MODEL)\n",
    "print(texts_embedded.embeddings[0][:5]) # Let's check embeddings for the first text\n",
    "\n",
    "# create a list of json objects with {id, title, text, emb}\n",
    "json_docs = []\n",
    "for i in range(len(texts)):\n",
    "    json_docs.append({'id': i, 'title': titles[i], 'text': texts[i], 'emb': texts_embedded.embeddings[i]})\n",
    "    \n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open('data/Rules_2024_v1.1_embedding.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(json_docs, file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIlx5RVCP7g7"
   },
   "source": [
    "Create a tensor with the given embeddings and use it to search for similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBa3oxSsP2fv",
    "outputId": "d9d71135-7ac3-4424-d806-2a994a0b456a"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# open a file, where you stored the pickled data\n",
    "file = open('data/Rules_2024_v1.1_embedding.pkl', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "json_docs = pickle.load(file)\n",
    "\n",
    "# close the file\n",
    "file.close()\n",
    "\n",
    "\n",
    "docs = []\n",
    "doc_embeddings = []\n",
    "for doc in json_docs:\n",
    "    docs.append(doc)\n",
    "    doc_embeddings.append(doc['emb'])\n",
    "    if len(docs) >= max_docs:\n",
    "        print(\"Too many documents, breaking\")\n",
    "        break\n",
    "\n",
    "doc_embeddings = torch.tensor(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbYAXaI4RQiH"
   },
   "source": [
    "To search, we embed the query, then get the nearest neighbors to its embedding (using dot product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "SJGUurziNiYR",
    "outputId": "91f54675-8bee-4cc7-a04b-152839cc75ec"
   },
   "outputs": [],
   "source": [
    "def calculateQuery(co, query, model):\n",
    "    response = co.embed(texts=[query], model=MODEL)\n",
    "    query_embedding = response.embeddings\n",
    "    query_embedding = torch.tensor(query_embedding)\n",
    "\n",
    "    # Compute dot score between query embedding and document embeddings\n",
    "    dot_scores = torch.mm(query_embedding, doc_embeddings.transpose(0, 1))\n",
    "    top_k = torch.topk(dot_scores, k=3)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nSimilar rules:\")\n",
    "    for doc_id in top_k.indices[0].tolist():\n",
    "        print(docs[doc_id]['title'])\n",
    "        print(docs[doc_id]['text'], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test our model with some queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \n",
      "What is the minimum required thickness for the scatter shield of the motor?\n",
      "\n",
      "\n",
      "Similar rules:\n",
      "T 7.3.4\n",
      "The tractive electric motor(s) must have a housing or separate scatter shield from nonperforated 2 mm aluminium alloy 6061-T6 or equivalent. The scatter shield may be split into\n",
      "two equal sections, each 1 mm thick. \n",
      "\n",
      "T 9.1.9\n",
      "Gas cylinders/tanks and their pressure regulators must be shielded from the driver. The\n",
      "shields must be steel or aluminium with a minimum thickness of 1 mm. \n",
      "\n",
      "T 7.3.2\n",
      "Exposed rotating final drivetrain parts, such as gears, clutches, chains and belts must be fitted\n",
      "with scatter shields. Scatter shields and their mountings must: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the query, then embed it\n",
    "query = \"\"\"\n",
    "What is the minimum required thickness for the scatter shield of the motor?\n",
    "\"\"\"   \n",
    "\n",
    "calculateQuery(co, query, MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
